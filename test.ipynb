{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the code on Surface Pro 3 core i5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "mat = cPickle.load(open(\"data/100000.dat\", 'rb')).astype(bool)\n",
    "\n",
    "if not mat.has_sorted_indices:\n",
    "    mat.sort_indices()\n",
    "    print \"all is sorted!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dictionary is done in 283 ms.\n",
      "Constructing adjacency matrix is done in 771 ms.\n",
      "DBSCANning is done in 4 ms.\n",
      "Restoring original point indices is done in 11 ms.\n",
      "\n",
      "Program finished after 1075 ms.\n",
      "\n",
      "Total number of clusters: 1692\n",
      "Largest cluster size is: 28470\n",
      "Noise size: 132\n"
     ]
    }
   ],
   "source": [
    "from dbscan import dbscan\n",
    "clusters = dbscan(mat, eps=.15, min_pts=2, report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 1.03 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit dbscan(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "Some performance improvement with Cython. You found the hidden gem, but it's only 20% speed boost. I casted memoryviews on csr_matrix raw data but it didn't help at all :/\n",
    "\n",
    "> This code is identical to the pure python version except where the variables are typed (cdef)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "from time import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "def dbscan(mat, eps=.15, min_pts=2, report=False):\n",
    "    '''\n",
    "    Density-based spatial clustering of applications with noise (DBSCAN).\n",
    "\n",
    "    :param mat: scipy csr_matrix format\n",
    "    :param eps: distance epsilon\n",
    "    :param min_pts: minimum number of points required to form a dense region\n",
    "    :param report: en/disable measuring duration of each process and reporting\n",
    "    :return: returns clusters\n",
    "    '''\n",
    "    # --------------------------------------------------------------------------\n",
    "    cdef long i, j, k, i_len, j_len\n",
    "\n",
    "    if report:\n",
    "        print 'Converting to dictionary',\n",
    "        init_t = time()\n",
    "\n",
    "    # @condensed:\n",
    "    # - keys: frozenset of the index of the nonzero entries of the matrix rows\n",
    "    # - values: list of the original row indices\n",
    "    # Note: frozenset is hashable and can be used as a key unlike regular python set.\n",
    "    # This is useful when obtaining the intersection in Jaccard Distance\n",
    "    # calculation.\n",
    "    condensed = defaultdict(list)\n",
    "\n",
    "    # Note: accessing raw data (indices and indptr attributes of csr_matrix)\n",
    "    # had 30x performance boost compared to normal indexing/slicing/iteration\n",
    "    # of sparse matrices.\n",
    "    j = 0\n",
    "    for i, k in enumerate(mat.indptr[1:]):\n",
    "        condensed[frozenset(mat.indices[j:k])].append(i)\n",
    "        j = k\n",
    "\n",
    "    if report:\n",
    "        print 'is done in %d ms.' % ((time() - init_t) * 1000)\n",
    "        # ----------------------------------------------------------------------\n",
    "        print 'Constructing adjacency matrix',\n",
    "        t = time()\n",
    "\n",
    "    # initializing the adjacency matrix\n",
    "    adjacency = [[] for i in range(len(condensed))]\n",
    "\n",
    "    keys = condensed.keys()\n",
    "    # sorting the points based on number of elements they contains\n",
    "    sorted_lens = Counter({i: len(key) for i, key in enumerate(keys)}).most_common()\n",
    "\n",
    "    # constructing adjacency matrix\n",
    "    for k, (i, i_len) in enumerate(sorted_lens):\n",
    "        # lengths[k + 1:]: iterating over half-diagonal, and also excluding the\n",
    "        # diagonal itself since they are alway true\n",
    "        for j, j_len in sorted_lens[k + 1:]:\n",
    "            # initial check assumming the best case scenario in which one of\n",
    "            # the points is subset of the other one. If this neighbor doesn't\n",
    "            # pass the test, the rest won't either because lengths are sorted\n",
    "            # so we BREAK the loop. This had 3x performance boost compared to\n",
    "            # unsorted length case.\n",
    "            if j_len < (1 - eps) * i_len:\n",
    "                break\n",
    "\n",
    "            # calculating proximity based on Jaccard distance\n",
    "            intersection = len(keys[i] & keys[j])\n",
    "\n",
    "            # from math: a U b = a + b - a I b\n",
    "            if 1 - 1.0 * intersection / (i_len + j_len - intersection) <= eps:\n",
    "                adjacency[i].append(j)\n",
    "                adjacency[j].append(i)\n",
    "\n",
    "    if report:\n",
    "        print 'is done in %d ms.' % ((time() - t) * 1000)\n",
    "        # ----------------------------------------------------------------------\n",
    "        print 'DBSCANning',\n",
    "        t = time()\n",
    "\n",
    "    # constructing noise cluster and marking them as visited\n",
    "    visited = [False] * len(condensed)\n",
    "    clusters = [[]]\n",
    "    for i, neighbors in enumerate(adjacency):\n",
    "        if len(neighbors) + len(condensed[keys[i]]) < min_pts:\n",
    "            clusters[0].append(i)\n",
    "            visited[i] = True\n",
    "\n",
    "    # searching for clusters\n",
    "    for i, neighbors in enumerate(adjacency):\n",
    "        if not visited[i]:\n",
    "            visited[i] = True\n",
    "            clusters.append(set([i]))\n",
    "            for j in neighbors:\n",
    "                if not visited[j]:\n",
    "                    visited[j] = True\n",
    "                    neighbors.extend([k for k in adjacency[j] if not visited[k]])\n",
    "\n",
    "                # index -1 is the last item in the list\n",
    "                clusters[-1].add(j)\n",
    "\n",
    "    if report:\n",
    "        print 'is done in %d ms.' % ((time() - t) * 1000)\n",
    "        # ----------------------------------------------------------------------\n",
    "        print 'Restoring original point indices',\n",
    "        t = time()\n",
    "\n",
    "    # restoring original indices that was lost when condensing the matrix in\n",
    "    # the beginning\n",
    "    clusters = [[i for j in c for i in condensed[keys[j]]] for c in clusters]\n",
    "\n",
    "    if report:\n",
    "        print 'is done in %d ms.' % ((time() - t) * 1000)\n",
    "        print '\\nProgram finished after %d ms.\\n' % ((time() - init_t) * 1000)\n",
    "\n",
    "        print 'Total number of clusters:', len(clusters)\n",
    "        print 'Largest cluster size is:', max(map(len, clusters[1:]))\n",
    "        print 'Noise size:', len(clusters[0])\n",
    "#         print 'Number of members in each cluster:', map(len, clusters[1:])\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dictionary is done in 253 ms.\n",
      "Constructing adjacency matrix is done in 529 ms.\n",
      "DBSCANning is done in 17 ms.\n",
      "Restoring original point indices is done in 11 ms.\n",
      "\n",
      "Program finished after 813 ms.\n",
      "\n",
      "Total number of clusters: 1692\n",
      "Largest cluster size is: 28470\n",
      "Noise size: 132\n"
     ]
    }
   ],
   "source": [
    "clusters = dbscan(mat, eps=.15, min_pts=2, report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 780 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit dbscan(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can improve the performance of this code, please share your tweaks.\n",
    "\n",
    "### Happy coding :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
