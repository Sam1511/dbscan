{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the code on Surface Pro 3 core i5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "mat = cPickle.load(open(\"data/100000.dat\", 'rb')).astype(bool)\n",
    "\n",
    "if not mat.has_sorted_indices:\n",
    "    mat.sort_indices()\n",
    "    print \"all is sorted!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dictionary is done in 0.3s.\n",
      "Constructing neighbors matrix is done in 0.8s.\n",
      "DBSCANning is done in 0.0s.\n",
      "Program finished after 1.2s.\n",
      "\n",
      "Largest cluster size is:\t28470\n",
      "Total number of clusters:\t1692\n",
      "Number of members in noise:\t132\n"
     ]
    }
   ],
   "source": [
    "from dbscan import dbscan\n",
    "dbscan(mat, eps=.15, min_pts=2, report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 1.1 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit dbscan(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "Some performance improvement with Cython. You found the hidden gem, but it's only 20% speed boost. I casted memoryviews on csr_matrix raw data but it didn't help at all :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "\n",
    "from time import gmtime, strftime, time\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def dbscan(mat, eps=.15, min_pts=2, report=False):\n",
    "    \"\"\"\n",
    "    Density-based spatial clustering of applications with noise (DBSCAN).\n",
    "\n",
    "    :param mat: scipy csr_matrix format\n",
    "    :param eps: distance epsilon\n",
    "    :param min_pts: minimum number of points required to form a dense region\n",
    "    :param report: en/disable measuring duration of each process and reporting\n",
    "    :return: returns nothing\n",
    "    \"\"\"\n",
    "    # --------------------------------------------------------------------------\n",
    "    cdef:\n",
    "        long i, i_len, i_neibor, i_neibor_len, i_neibor_neibor, j, idx\n",
    "    if report:\n",
    "        print \"Converting to dictionary\",\n",
    "        init_time = time()\n",
    "\n",
    "    # accessing raw data (indices and indptr attributes of csr_matrix) had 30x \n",
    "    # performance boost compared to normal indexing/slicing/iteration.\n",
    "    \n",
    "    # keys are frozenset of the index of the nonzero entries of the matrix\n",
    "    # rows. frozenset is hashable and can be used as the key unlike regular\n",
    "    # sets. values are the row index\n",
    "    dictionary = {}\n",
    "    i = 0\n",
    "    for j in mat.indptr[1:]:\n",
    "        key = frozenset(mat.indices[i:j])\n",
    "        i = j\n",
    "        if key in dictionary:\n",
    "            dictionary[key].add(i)\n",
    "        else:\n",
    "            dictionary[key] = set([i])\n",
    "\n",
    "    # this is used later for optimizing construction of neighbors matrix\n",
    "    lengths = Counter()\n",
    "    keys = dictionary.keys()\n",
    "    for i, key in enumerate(keys):\n",
    "        lengths[i] = len(key)\n",
    "\n",
    "    if report:\n",
    "        print \"is done in %.1fs.\" % (time() - init_time)\n",
    "        # ----------------------------------------------------------------------\n",
    "        print \"Constructing neighbors matrix\",\n",
    "        t = time()\n",
    "\n",
    "    # initializing neighbors\n",
    "    neighbors = [[] for i in range(len(dictionary))]\n",
    "\n",
    "    # constructing neighbors matrix\n",
    "    sorted_lengths = lengths.most_common()\n",
    "    for idx, (i, i_len) in enumerate(sorted_lengths):\n",
    "        # lengths[idx+1:]: iterating over half-diagonal, and also excluding the\n",
    "        # diagonal itself since they are alway true\n",
    "        for i_neibor, i_neibor_len in sorted_lengths[idx + 1:]:\n",
    "            # initial check assumming the best case scenario in which one of\n",
    "            # the points is subset of the other one. If this neighbor doesn't\n",
    "            # pass the test, the rest won't either because lengths are sorted\n",
    "            # so we BREAK the loop. This had 3x performance boost compared to\n",
    "            # unsorted length case.\n",
    "            if i_neibor_len < (1 - eps) * i_len:\n",
    "                break\n",
    "\n",
    "            # calculating proximity based on Jaccard distance\n",
    "            intersection = len(keys[i] & keys[i_neibor])\n",
    "            # from math: a U b = a + b - a I b\n",
    "            if 1 - 1.0 * intersection / (i_len + i_neibor_len - intersection) <= eps:\n",
    "                neighbors[i].append(i_neibor)\n",
    "                neighbors[i_neibor].append(i)\n",
    "\n",
    "    if report:\n",
    "        print \"is done in %.1fs.\" % (time() - t)\n",
    "        # ----------------------------------------------------------------------\n",
    "        print \"DBSCANning\",\n",
    "        t = time()\n",
    "\n",
    "    # constructing noise cluster\n",
    "    visited_noise = [False] * len(dictionary)\n",
    "    noise = set([])\n",
    "    for i, i_neighbors in enumerate(neighbors):\n",
    "        if len(i_neighbors) + len(dictionary[keys[i]]) < min_pts:\n",
    "            # Restoring original indices of the nodes\n",
    "            noise.update(dictionary[keys[i]])\n",
    "            visited_noise[i] = True\n",
    "\n",
    "    # searching for clusters\n",
    "    clusters = []\n",
    "    for i, i_neighbors in enumerate(neighbors):\n",
    "        if not visited_noise[i]:\n",
    "            visited_noise[i] = True\n",
    "            # dictionary[keys[i]] restores the original indices of the nodes\n",
    "            clusters.append(dictionary[keys[i]])\n",
    "            for i_neibor in i_neighbors:\n",
    "                if not visited_noise[i_neibor]:\n",
    "                    visited_noise[i_neibor] = True\n",
    "                    for i_neibor_neibor in neighbors[i_neibor]:\n",
    "                        if not visited_noise[i_neibor_neibor]:\n",
    "                            i_neighbors.append(i_neibor_neibor)\n",
    "                # index -1 is the last item in the list\n",
    "                clusters[-1].update(dictionary[keys[i_neibor]])\n",
    "\n",
    "    if report:\n",
    "        print \"is done in %.1fs.\" % (time() - t)\n",
    "        print \"Program finished after %.1fs.\\n\" % (time() - init_time)\n",
    "\n",
    "        print \"Largest cluster size is:\\t\", max(map(len, clusters))\n",
    "        print \"Total number of clusters:\\t\", len(clusters) + 1\n",
    "        print \"Number of members in noise:\\t\", len(noise)\n",
    "        # print \"Number of members in each cluster:\\t\", map(len, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to dictionary is done in 0.3s.\n",
      "Constructing neighbors matrix is done in 0.5s.\n",
      "DBSCANning is done in 0.0s.\n",
      "Program finished after 0.9s.\n",
      "\n",
      "Largest cluster size is:\t28470\n",
      "Total number of clusters:\t1692\n",
      "Number of members in noise:\t132\n"
     ]
    }
   ],
   "source": [
    "dbscan(mat, eps=.15, min_pts=2, report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 837 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit dbscan(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can improve the speed of this code, please share your tweaks.\n",
    "\n",
    "### Happy coding :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
